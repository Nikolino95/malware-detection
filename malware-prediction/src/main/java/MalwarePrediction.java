import java.io.IOException;
import java.util.*;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;

import org.apache.spark.api.java.function.FlatMapFunction;

import org.apache.spark.api.java.function.MapFunction;

import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;

import org.apache.spark.sql.*;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.types.*;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import java.nio.file.Path;
import java.io.*;



public class MalwarePrediction  {
    public static KafkaConsumer consumer;
    public static Path basePath = new File("").toPath();
    public static void main(String[] args) throws StreamingQueryException,IOException {

        String topic = args[0];//"topic1";//System.getenv("topic");
        String server = args[1];//"192.168.1.2:9092";//System.getenv("server");
        //String topic = "topic1";
        String modelPath =args[2];//"hdfs://localhost:9000/user/Nikola/nikola/malware-model"
        String path = args[3];//"hdfs://localhost:9000/user/Nikola/nikola/trainingdata.csv";//"C:\\Users\\Nikola\\IdeaProjects\\malware-prediction\\src\\data\\UNSW_NB15_training-set.csv";
        //String pathTest = "hdfs://localhost:9000/user/Nikola/nikola/testingdata.csv";//"C:\\Users\\Nikola\\IdeaProjects\\malware-prediction\\src\\data\\UNSW_NB15_testing-set.csv";

        SparkConf conf = new SparkConf().setAppName("MalwareDetection").setMaster("local[4]");
        conf.set("spark.driver.memory","524288000");
        conf.set("spark.executor.memory","524288000");
        conf.set("spark.testing.memory","524288000");
        SparkContext sc1= new SparkContext(conf);
        //JavaSparkContext jsc = new JavaSparkContext(conf);
        SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
        Dataset data= spark.read().format("csv").option("sep",",").option("inferSchema","true").option("header","true").load(path);
        Dataset<Row> dataset = data.cache();
        ////////////////////////////////////////////////////////////////////////////////////////////////////////////////
        //////////////////////////////////////////////////create model//////////////////////////////////////////////////
//        Dataset data= spark.read().format("csv").option("sep",",").option("inferSchema","true").option("header","true").load(path);
//        Dataset dataTest= spark.read().format("csv").option("sep",",").option("inferSchema","true").option("header","true").load(pathTest);
//        Dataset<Row> dataset = data.cache();
//        Dataset<Row> datasetTest = dataTest.cache();
//        List<String> columns = new ArrayList<String>(Arrays.asList(dataset.columns()));
//        columns.remove(columns.size()-1);
//        StringIndexer indexer = new StringIndexer().setInputCol("isMalware").setOutputCol("label");
//        StringIndexerModel ml = indexer.fit(dataset);
//        VectorAssembler assembler =  new VectorAssembler().setInputCols(columns.subList(0,columns.size()).stream().toArray(String[]::new)).setOutputCol("features");
//        RandomForestClassifier classifier = new RandomForestClassifier().setImpurity("gini").setMaxDepth(3).setNumTrees(20).setFeatureSubsetStrategy("auto");
////        dataset = assembler.transform(dataset);
////        datasetTest = assembler.transform(datasetTest);
//        Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {assembler,ml,classifier});
//        PipelineModel model = pipeline.fit(dataset);
//        model.write().overwrite().save("hdfs://localhost:9000/user/Nikola/nikola/malware-model");
//        Dataset<Row> result = model.transform(datasetTest);
//        result.show();
//
//        BinaryClassificationEvaluator evaluator = new BinaryClassificationEvaluator().setLabelCol("label").setMetricName("areaUnderROC");
//        System.out.println(evaluator.evaluate(result));
//        indexer.fit(dataset).transform(dataset).show();
        ////////////////////////////////////////////////////////////////////////////////////////////////////////////////
        List<String> columns = new ArrayList<String>(Arrays.asList(dataset.columns()));
        StructField[] fields = new StructField[42];
        for(int i = 0; i < 42; i++)
            fields[i] = new StructField(columns.get(i), DataTypes.DoubleType,true,Metadata.empty());
        StructType structType = new StructType(fields);
        PipelineModel model = PipelineModel.load(modelPath);
        Dataset<Row> datasetTest = spark.readStream().format("kafka").option("kafka.bootstrap.servers", server).option("subscribe", topic).load();
        Dataset<Row> dst = datasetTest.selectExpr("CAST(value AS STRING)").as(Encoders.STRING()).flatMap((FlatMapFunction<String, String>)x-> Arrays.asList(x.split("\n")).iterator(),Encoders.STRING())
                .map((MapFunction<String, Row>)y->{
                    Double[] values = new Double[42];
                    String[] stringValues = y.split(",");
                    for(int i=0;i<42;i++)
                        values[i]= Double.parseDouble(stringValues[i]);
                    return RowFactory.create(values);
                }, RowEncoder.apply(structType));
        Dataset<Row> result = model.transform(dst);
        StreamingQuery query = result.writeStream().outputMode("append").format("console").start();
        query.awaitTermination();
        //BinaryClassificationEvaluator evaluator = new BinaryClassificationEvaluator().setLabelCol("label").setMetricName("areaUnderROC");



    }
}
