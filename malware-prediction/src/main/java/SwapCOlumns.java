import org.apache.hadoop.io.Writable;
import org.apache.spark.ml.*;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.util.MLWritable;
import org.apache.spark.ml.util.MLWriter;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.StructType;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.UUID;

public class SwapCOlumns extends Transformer implements MLWritable {
    @Override
    public Dataset<Row> transform(Dataset<?> dataset) {
        List<String> columns = new ArrayList<String>(Arrays.asList(dataset.columns()));
        String label = columns.remove(columns.size()-2);
        columns.add(label);
        return dataset.select(columns.get(0),columns.subList(1,columns.size()).stream().toArray(String[]::new));
    }

    @Override
    public StructType transformSchema(StructType structType) {
        return structType;
    }

    @Override
    public Transformer copy(ParamMap paramMap) {
        return null;
    }

    @Override
    public String uid() {
        return UUID.randomUUID().toString();
    }


    @Override
    public MLWriter write() {
        return new MLWriter() {
            @Override
            public void saveImpl(String s) {

            }
        };
    }

    @Override
    public void save(String s) throws IOException {

    }

}
